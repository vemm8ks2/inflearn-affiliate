# scripts/src/scraper.py
"""
ì¸í”„ëŸ° ê°•ì˜ ìŠ¤í¬ë˜í•‘ ìŠ¤í¬ë¦½íŠ¸ (ê°œì„  ë²„ì „)
- ì•ˆì •ì ì¸ ì„ íƒì ì‚¬ìš©
- ë¡œê¹… ì‹œìŠ¤í…œ ì ìš©
- í•¨ìˆ˜ ë¶„ë¦¬ ë° ëª¨ë“ˆí™”
- ì„¤ì • ê´€ë¦¬ ê°œì„ 
"""

from playwright.sync_api import sync_playwright, Locator, TimeoutError as PlaywrightTimeoutError, Error as PlaywrightError
import json
import time
import re
from datetime import datetime, timezone
from typing import Dict, Optional, List, Any

# ë¡œì»¬ ëª¨ë“ˆ import
from src.logger_config import logger
from src.config import config
from src.db_utils import upsert_courses


# ============================================================================
# ë°ì´í„° ì¶”ì¶œ í—¬í¼ í•¨ìˆ˜ë“¤
# ============================================================================

def extract_with_fallback(link: Locator, selectors: List[str], validator=None) -> Optional[str]:
    """
    ì—¬ëŸ¬ ì„ íƒìë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ì‹œë„í•˜ì—¬ í…ìŠ¤íŠ¸ ì¶”ì¶œ

    Args:
        link: Playwright Locator ê°ì²´
        selectors: ì‹œë„í•  ì„ íƒì ë¦¬ìŠ¤íŠ¸
        validator: ì¶”ì¶œí•œ í…ìŠ¤íŠ¸ ê²€ì¦ í•¨ìˆ˜ (optional)

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
    """
    for selector in selectors:
        try:
            elem = link.locator(selector).first
            if elem:
                text = elem.text_content(timeout=config.ELEMENT_TIMEOUT)
                if text and text.strip():
                    text = text.strip()
                    if validator is None or validator(text):
                        return text
        except Exception as e:
            logger.debug(f"ì„ íƒì ì‹¤íŒ¨ ({selector}): {e}")
            continue
    return None


def extract_text_by_selector(
    entry_elem: Locator,
    selector: str,
    field_name: str,
    timeout: int = None
) -> Optional[str]:
    """
    ì…€ë ‰í„°ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œí•˜ëŠ” ê³µí†µ í•¨ìˆ˜ (íƒ€ì„ì•„ì›ƒ ì¬ì‹œë„ í¬í•¨)

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator
        selector: CSS ì…€ë ‰í„°
        field_name: í•„ë“œëª… (ë¡œê¹…ìš©)
        timeout: íƒ€ì„ì•„ì›ƒ (ê¸°ë³¸ê°’: config.ELEMENT_TIMEOUT)

    Returns:
        ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë˜ëŠ” None
    """
    timeout = timeout or config.ELEMENT_TIMEOUT

    # ì¬ì‹œë„ ë¡œì§: íƒ€ì„ì•„ì›ƒ ì‹œ ìµœëŒ€ MAX_RETRIESë²ˆ ì¬ì‹œë„
    for attempt in range(config.MAX_RETRIES + 1):
        try:
            elem = entry_elem.locator(selector).first
            if elem:
                value = elem.text_content(timeout=timeout)
                return value.strip() if value else None
        except PlaywrightTimeoutError:
            if attempt < config.MAX_RETRIES:
                logger.debug(f"{field_name} ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ - ì¬ì‹œë„ {attempt + 1}/{config.MAX_RETRIES}")
                time.sleep(config.RETRY_DELAY)
                continue
            else:
                logger.debug(f"{field_name} ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ (ì¬ì‹œë„ {config.MAX_RETRIES}íšŒ ì‹¤íŒ¨)")
        except AttributeError:
            logger.debug(f"{field_name} ìš”ì†Œ ì—†ìŒ (í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥)")
            break  # ì¬ì‹œë„ ë¶ˆí•„ìš”
        except PlaywrightError as e:
            logger.warning(f"{field_name} ì¶”ì¶œ ì‹¤íŒ¨ (Playwright ì˜¤ë¥˜): {e}")
            break  # ì¬ì‹œë„ ë¶ˆí•„ìš”
        except Exception as e:
            logger.error(f"{field_name} ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            break  # ì¬ì‹œë„ ë¶ˆí•„ìš”

    return None


def extract_title(entry_elem: Locator) -> Optional[str]:
    """
    ê°•ì˜ ì œëª© ì¶”ì¶œ

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator

    Returns:
        ì œëª© ë¬¸ìì—´ ë˜ëŠ” None
    """
    return extract_text_by_selector(entry_elem, config.SELECTORS['title'], "ì œëª©")


def clean_title(title: str) -> str:
    """
    ì œëª© ì •ì œ (ë¶ˆí•„ìš”í•œ ì ‘ë¯¸ì‚¬ ì œê±°)

    âœ… Phase 1.1: "ê°•ì˜ ì¸ë„¤ì¼" ë“± ë¶ˆí•„ìš”í•œ ì ‘ë¯¸ì‚¬ ì œê±°

    Args:
        title: ì›ë³¸ ì œëª© ë¬¸ìì—´

    Returns:
        ì •ì œëœ ì œëª© ë¬¸ìì—´
    """
    if not title:
        return title

    # ì œê±°í•  ì ‘ë¯¸ì‚¬ ëª©ë¡
    suffixes = ["ê°•ì˜ ì¸ë„¤ì¼", "ì¸ë„¤ì¼", "ê°•ì˜", " - "]

    cleaned = title.strip()
    for suffix in suffixes:
        if cleaned.endswith(suffix):
            cleaned = cleaned[:-len(suffix)].strip()

    return cleaned


def is_valid_instructor(instructor: str) -> bool:
    """
    ê°•ì‚¬ëª… ìœ íš¨ì„± ê²€ì¦

    Args:
        instructor: ê²€ì¦í•  ê°•ì‚¬ëª… ë¬¸ìì—´

    Returns:
        ìœ íš¨í•œ ê°•ì‚¬ëª…ì´ë©´ True, ì•„ë‹ˆë©´ False
    """
    if not instructor:
        return False

    # âœ… Phase 1.1: ê²€ì¦ ê°•í™” - ê´„í˜¸ í¬í•¨ ìˆ«ì ê±°ë¶€ (ë¦¬ë·° ìˆ˜: "(7)", "(244)")
    if re.match(r'^\(\d+\)$', instructor):
        logger.debug(f"ê°•ì‚¬ëª… ê²€ì¦ ì‹¤íŒ¨: ê´„í˜¸ í¬í•¨ ìˆ«ì (ë¦¬ë·° ìˆ˜ë¡œ ì¶”ì •) - '{instructor}'")
        return False

    # ìˆ«ìë§Œ ìˆìœ¼ë©´ ê±°ë¶€ (í‰ì  ì˜¤ì¸: "4.9", "5.0" ë“±)
    if re.match(r'^\d+(\.\d+)?$', instructor):
        logger.debug(f"ê°•ì‚¬ëª… ê²€ì¦ ì‹¤íŒ¨: ìˆ«ìë§Œ í¬í•¨ (í‰ì ìœ¼ë¡œ ì¶”ì •) - '{instructor}'")
        return False

    # í¼ì„¼íŠ¸ í¬í•¨ ê±°ë¶€ (í• ì¸ìœ¨ ì˜¤ì¸: "35%", "25%" ë“±)
    if '%' in instructor:
        logger.debug(f"ê°•ì‚¬ëª… ê²€ì¦ ì‹¤íŒ¨: í¼ì„¼íŠ¸ í¬í•¨ (í• ì¸ìœ¨ë¡œ ì¶”ì •) - '{instructor}'")
        return False

    # âœ… Phase 1.2: ê²€ì¦ ê°•í™” - ì›í™” ì‹¬ë³¼ ê±°ë¶€ (ê°€ê²©: "â‚©165,000", "â‚©31,460")
    if 'â‚©' in instructor:
        logger.debug(f"ê°•ì‚¬ëª… ê²€ì¦ ì‹¤íŒ¨: ì›í™” ì‹¬ë³¼ í¬í•¨ (ê°€ê²©ìœ¼ë¡œ ì¶”ì •) - '{instructor}'")
        return False

    # âœ… Phase 1.1: ê²€ì¦ ê°•í™” - "ì¼ë§Œ" íŒ¨í„´ ê±°ë¶€ (í• ì¸ ê¸°ê°„: "8ì¼ë§Œ", "6ì¼ë§Œ")
    if re.search(r'\d+ì¼ë§Œ', instructor):
        logger.debug(f"ê°•ì‚¬ëª… ê²€ì¦ ì‹¤íŒ¨: í• ì¸ ê¸°ê°„ íŒ¨í„´ - '{instructor}'")
        return False

    # âœ… Phase 1.1: ê²€ì¦ ê°•í™” - ë‹¨ì¼ ê¸°ìˆ  ì´ë¦„ ê±°ë¶€ (ì¹´í…Œê³ ë¦¬: "C++", "Java")
    if not re.search(r'[ê°€-í£]', instructor) and len(instructor) <= 5:
        logger.debug(f"ê°•ì‚¬ëª… ê²€ì¦ ì‹¤íŒ¨: ê¸°ìˆ  ì´ë¦„ìœ¼ë¡œ ì¶”ì • - '{instructor}'")
        return False

    # ìµœì†Œ 2ì, ìµœëŒ€ 50ì (í•œê¸€ ì´ë¦„ 2-10ì, ì˜ë¬¸ ì´ë¦„ ë” ê¸´ ê²½ìš° ê³ ë ¤)
    if not (2 <= len(instructor) <= 50):
        logger.debug(f"ê°•ì‚¬ëª… ê²€ì¦ ì‹¤íŒ¨: ê¸¸ì´ ë¶€ì ì ˆ ({len(instructor)}ì) - '{instructor}'")
        return False

    return True


def extract_instructor(entry_elem: Locator) -> Optional[str]:
    """
    ê°•ì‚¬ëª… ì¶”ì¶œ

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator

    Returns:
        ê°•ì‚¬ëª… ë˜ëŠ” None
    """
    return extract_text_by_selector(entry_elem, config.SELECTORS['instructor'], "ê°•ì‚¬ëª…")


def extract_thumbnail(entry_elem: Locator) -> Optional[str]:
    """
    ì¸ë„¤ì¼ ì´ë¯¸ì§€ URL ì¶”ì¶œ

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator

    Returns:
        ì´ë¯¸ì§€ URL ë˜ëŠ” None
    """
    try:
        img_elem = entry_elem.locator('picture img').first
        if img_elem:
            img_src = img_elem.get_attribute('src')
            if img_src:
                return img_src
    except PlaywrightTimeoutError:
        logger.debug("ì¸ë„¤ì¼ ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ (ìš”ì†Œ ë¡œë“œ ì§€ì—°)")
    except AttributeError:
        logger.debug("ì¸ë„¤ì¼ ìš”ì†Œ ì—†ìŒ (í˜ì´ì§€ êµ¬ì¡° ë³€ê²½ ê°€ëŠ¥)")
    except PlaywrightError as e:
        logger.warning(f"ì¸ë„¤ì¼ ì¶”ì¶œ ì‹¤íŒ¨ (Playwright ì˜¤ë¥˜): {e}")
    except Exception as e:
        logger.error(f"ì¸ë„¤ì¼ ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)

    return None


def parse_price(price_text: str) -> int:
    """
    ê°€ê²© ë¬¸ìì—´ì„ ì •ìˆ˜ë¡œ ë³€í™˜

    Args:
        price_text: "â‚©99,000" í˜•ì‹ì˜ ë¬¸ìì—´

    Returns:
        ì •ìˆ˜ ê°€ê²© (ë¬´ë£ŒëŠ” 0)
    """
    if not price_text or price_text == 'ë¬´ë£Œ':
        return 0
    try:
        return int(price_text.replace('â‚©', '').replace(',', '').strip())
    except ValueError:
        return 0


def parse_price_to_number(price_text: str) -> Optional[int]:
    """
    ê°€ê²© ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜

    Args:
        price_text: ê°€ê²© ë¬¸ìì—´ (ì˜ˆ: "â‚©77,000", "77000ì›")

    Returns:
        ë³€í™˜ëœ ê°€ê²© (ìˆ«ì) ë˜ëŠ” None

    Examples:
        >>> parse_price_to_number("â‚©77,000")
        77000
        >>> parse_price_to_number("55,000ì›")
        55000
    """
    if not price_text:
        return None

    try:
        # ìˆ«ìê°€ ì•„ë‹Œ ëª¨ë“  ë¬¸ì ì œê±° (â‚©, ì›, ì‰¼í‘œ ë“±)
        clean_text = ''.join(char for char in price_text if char.isdigit())
        if clean_text:
            return int(clean_text)
    except (ValueError, AttributeError) as e:
        logger.debug(f"ê°€ê²© ë³€í™˜ ì‹¤íŒ¨ ('{price_text}'): {e}")

    return None


def parse_student_count(count_text: str) -> Optional[int]:
    """
    ìˆ˜ê°•ìƒ ìˆ˜ ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜

    Args:
        count_text: ìˆ˜ê°•ìƒ ìˆ˜ ë¬¸ìì—´ (ì˜ˆ: "3,800+", "200+")

    Returns:
        ë³€í™˜ëœ ìˆ˜ê°•ìƒ ìˆ˜ (ìˆ«ì) ë˜ëŠ” None

    Examples:
        >>> parse_student_count("3,800+")
        3800
        >>> parse_student_count("200+")
        200
    """
    if not count_text:
        return None

    try:
        # ìˆ«ìì™€ ì‰¼í‘œë§Œ ì¶”ì¶œ í›„ ì‰¼í‘œ ì œê±° ('+' ì œê±°)
        clean_text = ''.join(char for char in count_text if char.isdigit() or char == ',')
        clean_text = clean_text.replace(',', '')
        if clean_text:
            return int(clean_text)
    except (ValueError, AttributeError) as e:
        logger.debug(f"ìˆ˜ê°•ìƒ ìˆ˜ ë³€í™˜ ì‹¤íŒ¨ ('{count_text}'): {e}")

    return None


def parse_discount_rate(discount_text: str) -> int:
    """
    í• ì¸ìœ¨ ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜

    Args:
        discount_text: í• ì¸ìœ¨ ë¬¸ìì—´ (ì˜ˆ: "35%", "50%")

    Returns:
        ë³€í™˜ëœ í• ì¸ìœ¨ (ìˆ«ì, 0-100) ë˜ëŠ” 0

    Examples:
        >>> parse_discount_rate("35%")
        35
        >>> parse_discount_rate("50%")
        50
        >>> parse_discount_rate(None)
        0
    """
    if not discount_text:
        return 0

    try:
        # % ê¸°í˜¸ ì œê±° í›„ ìˆ«ìë§Œ ì¶”ì¶œ
        clean_text = ''.join(char for char in discount_text if char.isdigit())
        if clean_text:
            return int(clean_text)
    except (ValueError, AttributeError) as e:
        logger.debug(f"í• ì¸ìœ¨ ë³€í™˜ ì‹¤íŒ¨ ('{discount_text}'): {e}")

    return 0


def clean_course_url(url: str) -> str:
    """
    URLì—ì„œ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±° (ì¶”ì  íŒŒë¼ë¯¸í„° ì •ê·œí™”)

    Args:
        url: ì›ë³¸ URL (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° í¬í•¨ ê°€ëŠ¥)

    Returns:
        ì •ê·œí™”ëœ URL (ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°ë¨)

    Examples:
        >>> clean_course_url("https://www.inflearn.com/course/test?attributionToken=abc")
        "https://www.inflearn.com/course/test"
        >>> clean_course_url("https://www.inflearn.com/course/test")
        "https://www.inflearn.com/course/test"
    """
    if not url:
        return url

    # '?' ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ì¿¼ë¦¬ íŒŒë¼ë¯¸í„° ì œê±°
    return url.split('?')[0]


def extract_single_price_element(entry_elem: Locator, selector: str, field_name: str) -> Optional[str]:
    """
    ë‹¨ì¼ ê°€ê²© ìš”ì†Œë¥¼ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator
        selector: CSS ì…€ë ‰í„°
        field_name: í•„ë“œëª… (ë¡œê¹…ìš©)

    Returns:
        ì¶”ì¶œëœ ê°€ê²© í…ìŠ¤íŠ¸ ë˜ëŠ” None
    """
    try:
        elem = entry_elem.locator(selector).first
        if elem and elem.count() > 0:
            text = elem.text_content(timeout=config.ELEMENT_TIMEOUT)
            return text.strip() if text else None
    except PlaywrightTimeoutError:
        logger.debug(f"{field_name} ì¶”ì¶œ íƒ€ì„ì•„ì›ƒ")
    except Exception as e:
        logger.debug(f"{field_name} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
    return None


def extract_price_info(entry_elem: Locator) -> Dict[str, Optional[Any]]:
    """
    ê°€ê²© ì •ë³´ ì¶”ì¶œ (ê°œì„  ë²„ì „ - ìˆ«ì ë³€í™˜ ë° is_on_sale í”Œë˜ê·¸ í¬í•¨)

    í˜ì´ì§€ êµ¬ì¡°:
    - í• ì¸ ì—†ìŒ: second_priceë§Œ ì¡´ì¬ (ì •ê°€)
    - í• ì¸ ì¤‘: first_price (ì •ê°€) + second_price (í• ì¸ê°€) + discount_rate ì¡´ì¬

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator

    Returns:
        ê°€ê²© ì •ë³´ ë”•ì…”ë„ˆë¦¬
        {
            'original_price': int,
            'sale_price': int,
            'discount_rate': int,
            'is_on_sale': bool
        }
    """
    try:
        # Step 1: first_price ì¶”ì¶œ ì‹œë„ (í• ì¸ ì „ ê°€ê²©)
        first_price_text = extract_single_price_element(
            entry_elem,
            config.SELECTORS['first_price'],
            "ì •ê°€(í• ì¸ì „)"
        )

        # Step 2: first_price ì¡´ì¬ ì—¬ë¶€ì— ë”°ë¼ ë¶„ê¸°
        if first_price_text:
            # í• ì¸ ì¤‘: first_price=ì •ê°€, second_price=í• ì¸ê°€, discount_rate ì¶”ì¶œ
            sale_price_text = extract_single_price_element(
                entry_elem,
                config.SELECTORS['sale_price'],
                "í• ì¸ê°€"
            )
            discount_rate_text = extract_single_price_element(
                entry_elem,
                config.SELECTORS['discount_rate'],
                "í• ì¸ìœ¨"
            )

            original_price = parse_price_to_number(first_price_text)
            sale_price = parse_price_to_number(sale_price_text)

            return {
                'original_price': original_price,
                'sale_price': sale_price,
                'discount_rate': parse_discount_rate(discount_rate_text),
                'is_on_sale': True,
            }
        else:
            # í• ì¸ ì—†ìŒ: second_price=ì •ê°€
            regular_price_text = extract_single_price_element(
                entry_elem,
                config.SELECTORS['second_price'],
                "ì •ê°€"
            )

            regular_price = parse_price_to_number(regular_price_text)

            return {
                'original_price': regular_price,
                'sale_price': regular_price,  # í• ì¸ ì—†ìœ¼ë©´ ì •ê°€ì™€ ë™ì¼
                'discount_rate': 0,
                'is_on_sale': False,
            }

    except Exception as e:
        logger.error(f"ê°€ê²© ì •ë³´ ì¶”ì¶œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)

    # ì˜ˆì™¸ ë°œìƒ ì‹œ ê¸°ë³¸ êµ¬ì¡° ë°˜í™˜ (dict unpacking TypeError ë°©ì§€)
    return {
        'original_price': None,
        'sale_price': None,
        'discount_rate': 0,
        'is_on_sale': False,
    }


def extract_rating(entry_elem: Locator) -> Optional[float]:
    """
    í‰ì  ì¶”ì¶œ ë° float ë³€í™˜

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator

    Returns:
        í‰ì  (0-5) ë˜ëŠ” None
    """
    rating_text = extract_text_by_selector(entry_elem, config.SELECTORS['rating'], "í‰ì ")
    if rating_text:
        try:
            rating = float(rating_text.strip())
            # ë²”ìœ„ ê²€ì¦ (0-5)
            if 0 <= rating <= 5:
                return rating
            else:
                logger.warning(f"í‰ì  ë²”ìœ„ ì´ˆê³¼: {rating}")
        except ValueError as e:
            logger.debug(f"í‰ì  ë³€í™˜ ì‹¤íŒ¨ ('{rating_text}'): {e}")
    return None


def extract_review_count(entry_elem: Locator) -> Optional[int]:
    """
    ë¦¬ë·° ìˆ˜ ì¶”ì¶œ ë° int ë³€í™˜

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator

    Returns:
        ë¦¬ë·° ìˆ˜ ë˜ëŠ” None
    """
    count_text = extract_text_by_selector(entry_elem, config.SELECTORS['review_count'], "ë¦¬ë·° ìˆ˜")
    if count_text:
        try:
            # ê´„í˜¸ ë° ì‰¼í‘œ ì œê±°: "(1,234)" â†’ "1234"
            clean_text = count_text.strip().strip('()').replace(',', '')
            return int(clean_text)
        except ValueError as e:
            logger.debug(f"ë¦¬ë·° ìˆ˜ ë³€í™˜ ì‹¤íŒ¨ ('{count_text}'): {e}")
    return None


def extract_student_count(entry_elem: Locator) -> Optional[int]:
    """
    ìˆ˜ê°•ìƒ ìˆ˜ ì¶”ì¶œ ë° int ë³€í™˜

    Args:
        entry_elem: ê°•ì˜ ìš”ì†Œ Locator

    Returns:
        ìˆ˜ê°•ìƒ ìˆ˜ (ìˆ«ì) ë˜ëŠ” None

    Examples:
        "3,800+" â†’ 3800
        "200+" â†’ 200
    """
    count_text = extract_text_by_selector(entry_elem, config.SELECTORS['student_count'], "ìˆ˜ê°•ìƒ ìˆ˜")
    return parse_student_count(count_text)


def extract_course_data(link: Locator, idx: int) -> Dict[str, Any]:
    """
    ê°•ì˜ í•˜ë‚˜ì˜ ëª¨ë“  ë°ì´í„° ì¶”ì¶œ

    Args:
        link: ê°•ì˜ ë§í¬ Locator
        idx: ê°•ì˜ ì¸ë±ìŠ¤

    Returns:
        ê°•ì˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
    """
    try:
        # URL ë° course_id
        raw_url = link.get_attribute('href')
        url = clean_course_url(raw_url)  # ì¶”ì  íŒŒë¼ë¯¸í„° ì œê±°
        course_id = None

        if url:
            course_id = url.split('/course/')[-1]

        entry_elem = link.locator('div > div:nth-child(2) > div > article')

        # ëª¨ë“  í•„ë“œ ì¶”ì¶œ
        course = {
            'url': url,
            'course_id': course_id,
            'title': extract_title(entry_elem),
            'instructor': extract_instructor(entry_elem),
            'thumbnail_url': extract_thumbnail(entry_elem),
            **extract_price_info(entry_elem),
            'rating': extract_rating(entry_elem),
            'review_count': extract_review_count(entry_elem),
            'student_count': extract_student_count(entry_elem),
            'scraped_at': datetime.now(timezone.utc).isoformat(),
            'source': 'inflearn',
        }

        return course

    except Exception as e:
        logger.error(f"ê°•ì˜ {idx+1} ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨", exc_info=True)
        return {}


def is_valid_course(course: Dict[str, Any]) -> bool:
    """
    ê°•ì˜ ë°ì´í„° ìœ íš¨ì„± ê²€ì¦

    Args:
        course: ê°•ì˜ ë°ì´í„° ë”•ì…”ë„ˆë¦¬

    Returns:
        ìœ íš¨ ì—¬ë¶€
    """
    if not course.get('title') or not course.get('url'):
        return False

    # ì œëª© ê¸¸ì´ ê²€ì¦
    if len(course.get('title', '')) < 3:
        logger.warning(f"ì œëª©ì´ ë„ˆë¬´ ì§§ìŒ: {course.get('title')}")
        return False

    return True


def log_course_info(course: Dict[str, Any], idx: int):
    """
    ìˆ˜ì§‘í•œ ê°•ì˜ ì •ë³´ ë¡œê¹…

    Args:
        course: ê°•ì˜ ë°ì´í„°
        idx: ì¸ë±ìŠ¤
    """
    logger.info(f"  [{idx+1}] {course.get('title', 'N/A')[:40]}...")
    logger.info(f"       ê°•ì‚¬: {course.get('instructor', 'N/A')}")

    # ê°€ê²© ì •ë³´ ì¶œë ¥
    if course.get('sale_price'):
        logger.info(f"       ì •ê°€: {course.get('original_price', 'N/A')}, "
                   f"í• ì¸ê°€: {course.get('sale_price')}, "
                   f"í• ì¸ìœ¨: {course.get('discount_rate', 'N/A')}%")
    else:
        logger.info(f"       ê°€ê²©: {course.get('original_price', 'N/A')}")

    logger.info(f"       í‰ì : {course.get('rating', 'N/A')}, "
               f"ë¦¬ë·°: {course.get('review_count', 'N/A')}, "
               f"ìˆ˜ê°•ìƒ: {course.get('student_count', 'N/A')}")


# ============================================================================
# ë©”ì¸ ìŠ¤í¬ë˜í•‘ í•¨ìˆ˜
# ============================================================================

def load_course_list(page, url: str) -> List[Locator]:
    """
    í˜ì´ì§€ ë¡œë“œ ë° ê°•ì˜ ë§í¬ ìˆ˜ì§‘ (ìµœì í™”ëœ ëŒ€ê¸° ì „ëµ)

    Args:
        page: Playwright Page ê°ì²´
        url: ì ‘ì†í•  URL

    Returns:
        ê°•ì˜ ë§í¬ Locator ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"ğŸŒ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")
    page.goto(url, wait_until="domcontentloaded", timeout=config.PAGE_LOAD_TIMEOUT)

    # ë„¤íŠ¸ì›Œí¬ê°€ idle ìƒíƒœê°€ ë  ë•Œê¹Œì§€ ëŒ€ê¸° (ì¦ê°€ëœ íƒ€ì„ì•„ì›ƒ)
    try:
        page.wait_for_load_state('networkidle', timeout=10000)
        logger.debug("ë„¤íŠ¸ì›Œí¬ idle ë„ë‹¬")
    except PlaywrightTimeoutError:
        logger.debug("í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° íƒ€ì„ì•„ì›ƒ (ê³„ì† ì§„í–‰)")

    # ìŠ¤í¬ë¡¤í•˜ì—¬ ì½˜í…ì¸  ë¡œë“œ (ì¦ê°€ëœ íšŸìˆ˜ì™€ ì•ˆì •ì ì¸ ëŒ€ê¸°)
    logger.info("ğŸ“œ í˜ì´ì§€ ìŠ¤í¬ë¡¤ ì¤‘...")
    for i in range(5):
        page.evaluate("window.scrollBy(0, window.innerHeight)")
        time.sleep(1)  # ê° ìŠ¤í¬ë¡¤ í›„ 1ì´ˆ ëŒ€ê¸° (ì•ˆì •ì ì¸ ë Œë”ë§ ë³´ì¥)
        logger.debug(f"ìŠ¤í¬ë¡¤ {i+1}/5 ì™„ë£Œ")

    # ê°•ì˜ ë§í¬ ìˆ˜ì§‘
    logger.info("ğŸ” ê°•ì˜ ë§í¬ ìˆ˜ì§‘ ì¤‘...")
    course_links = page.locator(config.SELECTORS['course_link']).all()
    logger.info(f"âœ… {len(course_links)}ê°œì˜ ê°•ì˜ ë°œê²¬")

    return course_links


def extract_all_courses(course_links: List[Locator], max_courses: int, max_retries: int = 2) -> tuple[List[Dict], List[Dict]]:
    """
    ëª¨ë“  ê°•ì˜ ë°ì´í„° ì¶”ì¶œ (ë©”íŠ¸ë¦­ ìˆ˜ì§‘ ë° ì‹¤íŒ¨ ì¶”ì  í¬í•¨)

    Args:
        course_links: ê°•ì˜ ë§í¬ Locator ë¦¬ìŠ¤íŠ¸
        max_courses: ìˆ˜ì§‘í•  ìµœëŒ€ ê°•ì˜ ìˆ˜
        max_retries: ê°•ì˜ë³„ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ (ê¸°ë³¸ê°’: 2)

    Returns:
        tuple: (ê°•ì˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸, ì‹¤íŒ¨í•œ ê°•ì˜ ë¦¬ìŠ¤íŠ¸)
    """
    courses = []
    failed_courses = []
    metrics = {
        'total': 0,
        'success': 0,
        'validation_failed': 0,
        'extraction_failed': 0,
        'retried': 0
    }

    logger.info(f"ğŸ“Š ë°ì´í„° ì¶”ì¶œ ì¤‘ (ìµœëŒ€ {max_courses}ê°œ)...")
    start_time = time.time()

    for idx, link in enumerate(course_links[:max_courses]):
        metrics['total'] += 1
        retry_count = 0
        success = False
        last_error = None

        # ê°•ì˜ë³„ ì¬ì‹œë„ ë¡œì§
        while retry_count <= max_retries and not success:
            try:
                course_data = extract_course_data(link, idx)

                if is_valid_course(course_data):
                    courses.append(course_data)
                    log_course_info(course_data, idx)
                    metrics['success'] += 1
                    success = True
                else:
                    last_error = "Validation failed"
                    metrics['validation_failed'] += 1
                    break  # ê²€ì¦ ì‹¤íŒ¨ëŠ” ì¬ì‹œë„ ë¶ˆí•„ìš”

            except Exception as e:
                last_error = str(e)
                retry_count += 1

                if retry_count <= max_retries:
                    logger.warning(f"  [{idx+1}] âš ï¸  ì‹œë„ {retry_count}/{max_retries} ì‹¤íŒ¨: {e}")
                    metrics['retried'] += 1
                    time.sleep(config.RETRY_DELAY * retry_count)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    logger.error(f"  [{idx+1}] âŒ ìµœì¢… ì‹¤íŒ¨ (ì¬ì‹œë„ {max_retries}íšŒ ì´ˆê³¼)", exc_info=True)
                    metrics['extraction_failed'] += 1

        # ì‹¤íŒ¨í•œ ê°•ì˜ ê¸°ë¡
        if not success:
            failed_info = {
                "index": idx + 1,
                "error": last_error,
                "retry_count": retry_count,
                "url": None
            }

            # URL ì¶”ì¶œ ì‹œë„
            try:
                raw_url = link.get_attribute('href')
                if raw_url:
                    failed_info["url"] = clean_course_url(raw_url)
            except:
                pass

            failed_courses.append(failed_info)

    # ì‹¤íŒ¨ ëª©ë¡ ì €ì¥
    if failed_courses:
        from pathlib import Path
        failed_path = Path(__file__).parent.parent / "output" / "failed_courses.json"
        try:
            with open(failed_path, "w", encoding="utf-8") as f:
                json.dump(failed_courses, f, ensure_ascii=False, indent=2)
            logger.warning(f"âš ï¸  ì‹¤íŒ¨ ëª©ë¡ ì €ì¥: {len(failed_courses)}ê°œ - {failed_path}")
        except Exception as e:
            logger.error(f"ì‹¤íŒ¨ ëª©ë¡ ì €ì¥ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)

    # ë©”íŠ¸ë¦­ ìš”ì•½ ì¶œë ¥
    elapsed = time.time() - start_time
    success_rate = (metrics['success'] / metrics['total'] * 100) if metrics['total'] > 0 else 0
    logger.info(f"\n{'='*60}")
    logger.info(f"ğŸ“ˆ ìˆ˜ì§‘ í†µê³„:")
    logger.info(f"  â€¢ ì „ì²´: {metrics['total']}ê°œ")
    logger.info(f"  â€¢ ì„±ê³µ: {metrics['success']}ê°œ ({success_rate:.1f}%)")
    logger.info(f"  â€¢ ê²€ì¦ ì‹¤íŒ¨: {metrics['validation_failed']}ê°œ")
    logger.info(f"  â€¢ ì¶”ì¶œ ì‹¤íŒ¨: {metrics['extraction_failed']}ê°œ")
    logger.info(f"  â€¢ ì¬ì‹œë„: {metrics['retried']}íšŒ")
    logger.info(f"  â€¢ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")
    logger.info(f"  â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {elapsed/metrics['total']:.2f}ì´ˆ/ê°•ì˜" if metrics['total'] > 0 else "  â€¢ í‰ê·  ì²˜ë¦¬ ì‹œê°„: N/A")
    logger.info(f"{'='*60}\n")

    return courses, failed_courses


def save_debug_files(page):
    """
    ë””ë²„ê·¸ìš© íŒŒì¼ ì €ì¥

    Args:
        page: Playwright Page ê°ì²´
    """
    logger.info("ğŸ’¾ ë””ë²„ê·¸ íŒŒì¼ ì €ì¥ ì¤‘...")

    # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
    page.screenshot(path=config.SCREENSHOT_PATH)
    logger.info(f"ìŠ¤í¬ë¦°ìƒ· ì €ì¥: {config.SCREENSHOT_PATH}")

    # HTML ì†ŒìŠ¤ ì €ì¥
    html_content = page.content()
    with open(config.HTML_SOURCE_PATH, "w", encoding="utf-8") as f:
        f.write(html_content)
    logger.info(f"HTML ì†ŒìŠ¤ ì €ì¥: {config.HTML_SOURCE_PATH}")


def scrape_inflearn_courses(max_courses: Optional[int] = None, headless: Optional[bool] = None) -> tuple[List[Dict], Dict]:
    """
    ì¸í”„ëŸ° ê°•ì˜ ëª©ë¡ ìŠ¤í¬ë˜í•‘ (ë¦¬íŒ©í† ë§ ë²„ì „ - ë©”íƒ€ë°ì´í„° í¬í•¨)

    Args:
        max_courses: ìˆ˜ì§‘í•  ìµœëŒ€ ê°•ì˜ ìˆ˜ (ê¸°ë³¸ê°’: config.MAX_COURSES)
        headless: ë¸Œë¼ìš°ì € ìˆ¨ê¹€ ëª¨ë“œ (ê¸°ë³¸ê°’: config.HEADLESS)

    Returns:
        tuple: (ê°•ì˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸, ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬)
    """
    # ê¸°ë³¸ê°’ ì„¤ì •
    max_courses = max_courses if max_courses is not None else config.MAX_COURSES
    headless = headless if headless is not None else config.HEADLESS

    # ìŠ¤í¬ë˜í•‘ ì‹œì‘ ì‹œê°„ ê¸°ë¡
    start_time = time.time()
    start_datetime = datetime.now(timezone.utc)

    logger.info("=" * 60)
    logger.info("ğŸš€ ì¸í”„ëŸ° ê°•ì˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
    logger.info(f"ì„¤ì •: max_courses={max_courses}, headless={headless}")
    logger.info("=" * 60)

    with sync_playwright() as p:
        # ë¸Œë¼ìš°ì € ì‹¤í–‰ (ë´‡ íƒì§€ ìš°íšŒ ì„¤ì • ì¶”ê°€)
        browser = p.chromium.launch(headless=headless)
        context = browser.new_context(
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            viewport={'width': 1920, 'height': 1080},
            locale='ko-KR',
            extra_http_headers={
                'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            }
        )
        page = context.new_page()

        # í˜ì´ì§€ ë ˆë²¨ì—ì„œë„ Accept-Language í—¤ë” ëª…ì‹œì  ì„¤ì •
        page.set_extra_http_headers({
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        })

        try:
            # í˜ì´ì§€ ë¡œë“œ ë° ê°•ì˜ ë§í¬ ìˆ˜ì§‘
            url = f"{config.BASE_URL}/{config.CATEGORY}"
            course_links = load_course_list(page, url)

            # ëª¨ë“  ê°•ì˜ ë°ì´í„° ì¶”ì¶œ (ì‹¤íŒ¨ ì¶”ì  í¬í•¨)
            courses, failed_courses = extract_all_courses(course_links, max_courses)

            # ë””ë²„ê·¸ íŒŒì¼ ì €ì¥
            save_debug_files(page)

            # ìŠ¤í¬ë˜í•‘ ì¢…ë£Œ ì‹œê°„ ê³„ì‚°
            end_time = time.time()
            duration = round(end_time - start_time, 2)

            # ë©”íƒ€ë°ì´í„° ìƒì„±
            metadata = {
                "version": "1.0.0",
                "scraper_version": "2.2.0",  # ì‹¤íŒ¨ ì¶”ì  ë° ì¬ì‹œë„ ë¡œì§ ì¶”ê°€
                "total_courses": len(courses),
                "failed_courses": len(failed_courses),
                "scraped_at": start_datetime.isoformat(),
                "scraping_duration_seconds": duration,
                "config": {
                    "max_courses": max_courses,
                    "category": config.CATEGORY,
                    "headless": headless,
                    "base_url": config.BASE_URL
                }
            }

            logger.info(f"\nâœ… ì´ {len(courses)}ê°œ ê°•ì˜ ìˆ˜ì§‘ ì™„ë£Œ (ì†Œìš” ì‹œê°„: {duration}ì´ˆ)")
            return courses, metadata

        except Exception as e:
            logger.error(f"ìŠ¤í¬ë˜í•‘ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ: {e}", exc_info=True)
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ë©”íƒ€ë°ì´í„° ë°˜í™˜
            end_time = time.time()
            duration = round(end_time - start_time, 2)
            metadata = {
                "version": "1.0.0",
                "scraper_version": "2.1.0",
                "total_courses": 0,
                "scraped_at": start_datetime.isoformat(),
                "scraping_duration_seconds": duration,
                "config": {
                    "max_courses": max_courses,
                    "category": config.CATEGORY,
                    "headless": headless,
                    "base_url": config.BASE_URL
                },
                "error": str(e)
            }
            return [], metadata

        finally:
            context.close()
            browser.close()
            logger.debug("ë¸Œë¼ìš°ì € ì¢…ë£Œ")


def save_to_json(courses: List[Dict], metadata: Optional[Dict] = None, filename: Optional[str] = None):
    """
    ìˆ˜ì§‘í•œ ê°•ì˜ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)

    Args:
        courses: ê°•ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
        metadata: ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬ (ì„ íƒì )
        filename: ì €ì¥í•  íŒŒì¼ ê²½ë¡œ (ê¸°ë³¸ê°’: config.JSON_OUTPUT)
    """
    filename = filename or config.JSON_OUTPUT

    try:
        # ë©”íƒ€ë°ì´í„°ê°€ ìˆìœ¼ë©´ êµ¬ì¡°í™”ëœ í˜•ì‹ìœ¼ë¡œ ì €ì¥
        if metadata:
            output_data = {
                "metadata": metadata,
                "courses": courses
            }
        else:
            # í•˜ìœ„ í˜¸í™˜ì„±: ë©”íƒ€ë°ì´í„° ì—†ìœ¼ë©´ ê¸°ì¡´ í˜•ì‹ ìœ ì§€
            output_data = courses

        with open(filename, "w", encoding="utf-8") as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
    except Exception as e:
        logger.error(f"JSON ì €ì¥ ì‹¤íŒ¨: {e}", exc_info=True)


def print_summary(courses: List[Dict]):
    """
    ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½ ì¶œë ¥

    Args:
        courses: ê°•ì˜ ë°ì´í„° ë¦¬ìŠ¤íŠ¸
    """
    if not courses:
        logger.warning("âŒ ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    logger.info("\nğŸ“Š ìˆ˜ì§‘ ê²°ê³¼ ìš”ì•½:")
    logger.info(f"  - ì´ ê°•ì˜ ìˆ˜: {len(courses)}")
    logger.info(f"  - ì œëª© ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('title'))}")
    logger.info(f"  - ê°•ì‚¬ëª… ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('instructor'))}")
    logger.info(f"  - URL ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('url'))}")
    logger.info(f"  - ì¸ë„¤ì¼ ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('thumbnail_url'))}")
    logger.info(f"  - ì •ê°€ ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('original_price'))}")
    logger.info(f"  - í• ì¸ê°€ ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('sale_price'))}")
    logger.info(f"  - í• ì¸ìœ¨ ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('discount_rate'))}")
    logger.info(f"  - í‰ì  ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('rating'))}")
    logger.info(f"  - ë¦¬ë·° ìˆ˜ ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('review_count'))}")
    logger.info(f"  - ìˆ˜ê°•ìƒ ìˆ˜ ìˆëŠ” ê°•ì˜: {sum(1 for c in courses if c.get('student_count'))}")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰ (ë©”íƒ€ë°ì´í„° í¬í•¨)
        courses, metadata = scrape_inflearn_courses()

        if courses:
            # JSON ì €ì¥ (ë©”íƒ€ë°ì´í„° í¬í•¨)
            save_to_json(courses, metadata)

            # Supabase ì €ì¥
            logger.info("\nğŸ’¾ Supabase ì €ì¥ ì¤‘...")
            saved_count = upsert_courses(courses)

            # ê²°ê³¼ ìš”ì•½
            print_summary(courses)

            # ë©”íƒ€ë°ì´í„° ìš”ì•½ ì¶œë ¥
            logger.info("\nğŸ“‹ ë©”íƒ€ë°ì´í„°:")
            logger.info(f"  - ë°ì´í„° ë²„ì „: {metadata['version']}")
            logger.info(f"  - ìŠ¤í¬ë˜í¼ ë²„ì „: {metadata['scraper_version']}")
            logger.info(f"  - ìˆ˜ì§‘ ì‹œê°„: {metadata['scraped_at']}")
            logger.info(f"  - ì†Œìš” ì‹œê°„: {metadata['scraping_duration_seconds']}ì´ˆ")

            # ìµœì¢… ê²°ê³¼ ìš”ì•½
            logger.info("\nğŸ“Š ìµœì¢… ê²°ê³¼:")
            logger.info(f"  - ìˆ˜ì§‘: {len(courses)}ê°œ")
            logger.info(f"  - ì €ì¥: {saved_count}ê°œ")
            logger.info(f"  - ì‹œê°„: {datetime.now()}")
        else:
            logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

        logger.info("\n" + "=" * 60)
        logger.info("âœ… ì‘ì—… ì™„ë£Œ")
        logger.info("=" * 60)

    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        logger.error(f"ë©”ì¸ í•¨ìˆ˜ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}", exc_info=True)


if __name__ == "__main__":
    main()
